Production Migration: 
  ☐ ToDo: Create Case for Refund @created(24-03-18 11:39)
  ☐ ToDo: Complete Production Candidate paperwork Aaron forwarded @created(24-03-18 11:39)


Future: 
  ☐ ToDo: Add UnZip capability @created(24-02-14 12:12) @started(24-02-14 12:12)
  ☐ ToDo: CleanUp/ReQueue capability for TricklerCache bucket @created(24-03-18 11:38)
  ☐ ToDo: CleanUp/ReQueue capability for Tricklercache-process bucket @created(24-03-18 11:38)

Documentation DropBucket: 
  ToDo: Word Template

Documentation Data Cookbook - Ingesting and Transforming Data using S3 DropBucket and other methods:
  ✔ ToDo: Word Template @created(24-03-06 20:53) @done(24-03-06 20:53)
  ToDo: OPs Chapter
  ToDo: Troubleshooting Chapter 
  ToDo: Core Setup/Configuration Options 
  ToDo: Customer Configuration Options
  ToDo: Helpful AWS CLI Commands

DropBucket S3 Processor: 
  ✔ ToDo: Confirm Relational Table update is correct/works @created(24-03-06 20:49) @done(24-03-18 11:40)
  ☐ ToDo: Confirm Keyed DB update is correct/works @created(24-03-06 20:49)
  ☐ ToDo: Confirm NonKeyed DB update is correct/works @created(24-03-06 20:49)

  ✔ ToDo: Fix messaging for Firehose results @created(24-03-06 17:45) @done(24-03-12 22:38)

  ✔ Debug: fix file naming 1_1 for Recs and Batch @created(24-03-06 17:44) @done(24-03-18 11:40)

  ✔ Debug: Unknown application error occurred @created(24-03-06 11:45) @done(24-03-06 20:49)

  ☐ Debug: S3 Event delivered to Lambda but Lambda then reports File Key Not Found.... @created(24-01-31 12:35)
    Turns out S3 allows duplicate Objects without means to delete a specific Object (ETag and Version not working well at all) 
    So, for previously deleted the SDK makes it look like current referenced Object does not exist. 

  ☐ Debug: SQS Event in Batch, file processed and deleted Same event in Batch again, but file already deleted. @created(24-01-31 12:36)
    Same as previous, 
    Turns out S3 allows duplicate Objects without means to delete a specific Object (ETag and Version not working well at all) 
    So, for previously deleted SDK makes it look like current Object does not exist. 

  ☐ Debug: DropBucket file deleted after Exception processing the file, @created(24-01-31 12:36)

  ✔ Debug: VisualCrossing not updating @created(24-01-31 12:36) @done(24-02-14 12:10)

  ✘ Debug: Resolve SaveSample exception @created(24-01-31 12:36) @cancelled(24-03-18 11:41)
    ✘ `   2024-01 - 31T02: 18: 59.176Z	624ac595 - 1a20 - 4fc8 - 9919 -09c4d51cb07c	ERROR	Unhandled Promise Rejection @created(24-03-18 11:41) @cancelled(24-03-18 11:41)
✘ @created(24-03-18 11:41) @cancelled(24-03-18 11:41)

    ✔ ToDo: Confirm Retry of SQS Events (When Campaign throttles) works @created(24-02-03 18:24) @done(24-02-03 18:24)

   ✔ Todo: Report on all Objects that get "Unhandled Promise Rejection" Errors: @created(24-02-02 13:47) @done(24-03-12 22:50)
      ✔ The number of Updates in this batch Exceeds Max Row Updates allowed 101 in the Customers Config", @created(24-02-02 13:47) @done(24-03-12 22:50)
        ✔ *Can see "Unhandled" in Logs* @created(24-02-02 13:47) @done(24-03-12 22:50)


  ✔ ToDo: Add JSON Path to Map capability for JSON files in DropBucket @created(24-01-31 12:55) @started(24-02-14 12:12) @done(24-03-18 11:41) @lasted(4w4d22h29m56s)
  
  ☐ Todo: Test JSONPath capabilty @created(24-02-14 12:11)
  ✔ ToDo: - as of 10/2023 CSV handled as the papaparse engine handles the record boundary, @created(24-01-31 12:31) @done(24-01-31 12:31)
  ✔ ToDo:  Now need to solve for JSON content @created(24-01-31 12:31) @done(24-01-31 12:31)


  ✔ ToDo: Of concern, very large data sets @created(24-01-31 12:55) @done(24-03-12 22:39)

  ✔ ToDo: But how to parse LARGE JSON Files - each chunk for JSON content as each chunk is a @created(24-01-31 12:55) @done(24-03-06 11:46)
        ✔ network chunk that can land on any or no record boundary. @created(24-03-12 22:39) @done(24-03-12 22:39)
            ✔ Use a JSON Parser that handles record boundary just like the CSV parser? @created(24-03-12 22:39) @done(24-03-12 22:39)
            ✔ Parse out individual Updates from the JSON in the Read Stream using start/stop index @created(24-03-12 22:39) @done(24-03-12 22:39)
                ✔ of the stream/content? @created(24-03-12 22:39) @done(24-03-12 22:39)

  ToDo: Test NodeJS stream UnArchiver



DropBucket Queue Processor:

    ✔ Todo: Currently throttled to a single event each time, Add processing of multiple Events per invocation @created(24-02-14 12:04) @done(24-02-14 12:04)
    ✔ ToDo: Process to ReParse Cache and create Work Files @created(24-02-03 18:29) @done(24-02-03 18:29)
    
    ToDo: Column mapping - Inbound Column Name to Table Column Mapping
    ToDo: Type Validation - Inbound Data type validation to Mapping Type
        ✔ ToDo: Confirm SQS Queue deletes queued work @created(24-02-03 18:24) @done(24-02-03 18:24)

    ✔ //ToDo: Resolve the progression of these steps, currently "Completed" is logged regardless of 'completion' @created(24-03-12 22:42) @done(24-03-12 22:42)
    ✔ // and Delete happens regardless of 'completion' being successful or not @created(24-03-12 22:42) @done(24-03-12 22:42)
  ✔ ToDo: Confirm BatchItemFails are returned to Queue @created(24-01-31 12:55) @done(24-03-12 22:42)
        ✔ Should be returned to Queue for retry @created(24-03-12 22:42) @done(24-03-12 22:42)
  ✔ ToDo: Check on recovery from Invalid Access/Expired Token Exceptions @created(24-01-31 12:55) @done(24-03-12 22:42)
        ✔ "access token has expired" @created(24-03-12 22:42) @done(24-03-12 22:42)
        ✔ Should be returned to Queue for retry @created(24-03-12 22:42) @done(24-03-12 22:42)
  ✔ ToDo: Check on recovery/reporting for "Error Saving Row" Errors @created(24-01-31 12:55) @done(24-03-12 22:41)
✔ `<FAILURE failure_type="transient" description="Error saving row"> @created(24-03-12 22:41) @done(24-03-12 22:41)
  ✔ <COLUMN name="EventValue" > <![CDATA[renee.lankford@gmail.com]]> </COLUMN> @created(24-03-12 22:41) @done(24-03-12 22:41)
      ✔ < COLUMN name = "EventSource" > <![CDATA[track Event]]> </COLUMN> @created(24-03-12 22:41) @done(24-03-12 22:41)
          ✔ < COLUMN name = "Email" > <![CDATA[renee.lankford@gmail.com]]> </COLUMN> @created(24-03-12 22:41) @done(24-03-12 22:41)
              ✔ < COLUMN name = "EventTimestamp" > <![CDATA[2024-01-18T15: 45: 31.037Z]]> </COLUMN> @created(24-03-12 22:41) @done(24-03-12 22:41)
                  ✔ < COLUMN name = "EventName" > <![CDATA[email]]> </COLUMN> @created(24-03-12 22:41) @done(24-03-12 22:41)
                      ✔ < /FAILURE> @created(24-03-12 22:41) @done(24-03-12 22:41)
  `
  ✔ ToDo: Check on recovery/retry for ECONNRESET errors @created(24-01-31 12:55) @done(24-03-12 22:43)
        ✔ Should be returned to Queue for retry @created(24-03-12 22:43) @done(24-03-12 22:43)
✔ ` eventTimestamp @created(24-03-12 22:43) @done(24-03-12 22:43)
  ✔ 2024-01 - 20T17: 10: 54.871-05:00 @created(24-03-12 22:43) @done(24-03-12 22:43)
  ✔ logEvent @created(24-03-12 22:43) @done(24-03-12 22:43)
  ✔ 2024-01 - 20T22: 10: 54.871Z	589a5cd9 - 6126 - 505e-89cd-02ff260c8c9a	ERROR	Exception processing a Work File(process_0_pura_2024_01_20T21_25_08_509Z.csv - @created(24-03-12 22:43) @done(24-03-12 22:43)
      ✔ Error- Exception during getAccessToken: @created(24-03-12 22:43) @done(24-03-12 22:43)
      ✔ FetchError: request to https://api-campaign-us-2.goacoustic.com/oauth/token @created(24-03-12 22:43) @done(24-03-12 22:43)
            ✔ failed, reason: read ECONNRESET @created(24-03-12 22:43) @done(24-03-12 22:43)
  `        


Dropbucket SFTP Connector:
  ✔ ToDo: create another lambda for SFTP Work @created(24-03-18 11:43) @done(24-03-18 11:43)
  ☐ ToDo: Complete initial SFTP support @created(24-03-18 11:43)
  ☐ ToDo: Look into using AWS Transfer Family Solution (SFTP, et. al.) @created(24-03-06 20:51)
  ☐ ToDo: Extract Data from the SFTP Download folder @created(24-03-06 20:51)

DropBucket - API Periodic: 
  ToDo: Periodic URL call - pull data - VisualCrossing Weather Data
    Notes: Using AWS Example - 

  DropBucket UI Interface: 
  ☐ ToDo: Stand up an Interface to View and Edit Customer Configs @created(24-01-31 12:54)
  ☐ Todo: Stand up a Reporting Interface @created(24-01-31 12:25)
  ☐ ToDo: Interface to view Logs/Errors (echo cloudwatch logs?) @created(24-01-31 12:55)
  ☐ ToDo: Keep an eye on Anomaly Detection Reports @created(24-02-02 13:49)
  ☐ ToDo: Add DeadLetterQueue Report - need a report to list all DLQ Events in order to @created(24-01-31 12:55)
    be able to work / troubleshoot the issue




SegmentConnector: 
  ✔ Todo: Complete Test of latest @created(24-02-14 12:03) @done(24-02-14 12:03)
  ✔ Todo: Create PR @created(24-02-14 12:03) @done(24-02-14 12:03)
  ✔ ToDo: Confirm last update data arriving, no issues, @created(24-03-12 22:37) @done(24-03-12 22:37)
  ToDo: review all material for Catalog PR 
  ToDo: Create PR for Catalog Entry
  ToDo: Confirm Catalog Entry correctness and completion


Userguide to ToDo:             
      Projects:
  ☐ Anything with a colon at the end of the line is a project
  ☐ Projects will show some statistics next to them @1h
    ✔ By default those statistics are the number of pending todos and the sum of their time estimates @30m
  Nested:
    ☐ You can nest projects inside each other and fold them
    
  Todos:
  You can write plain text notes/descriptions wherever you want
  New:
    ☐ Press Cmd/Ctrl+Enter to add a new todo
  Done:
    ✔ Press Alt+D to mark a todo as done
    ✔ Press it again to undo the action
  Cancelled:
    ✘ Press Alt+C to mark a todo as cancelled
    ✘ Press it again to undo the action
  Tagging:
    ☐ You can add tags using the @ symbol, like this @tag
    ☐ There are some special, customizable tags: @critical @high @low @today
  Timekeeping:
    ✔ Completed todos can show a timestamp @done(17-11-03 10:42)
    ☐ Press Alt+S to mark a todo as started @started(17-11-03 10:42)
      ✔ Now it will show the elapsed time @started(17-11-03 10:42) @done(17-11-03 20:11) @lasted(9h29m)
    ☐ You can provide time estimates for your todos @1h30m
      ☐ We are even doing some natural language processing @est(1 day and 20 minutes)

      Formatting:
  You can format text in a markdown-like fashion
  Bold:
    ☐ Use asterisks for *bold*
  Italic:
    ☐ Use underscores for _italic_
  Strikethrough:
    ☐ Use tildes for ~strikethrough~
  Code:
    ☐ Use backticks for `code`

      Archive:
  ✔ You can archive finished todos here
  ✔ Congratulations, you are now a Todo+ master!

